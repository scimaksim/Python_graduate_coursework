{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example to show word count using spark SQL\n",
    "\n",
    "Get things set up.  On windows so I need these extra paths to be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/30 10:51:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Create a Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.functions import explode, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data from a local source.  End up with a spark SQL data frame with one column (value) and one entry that is the text of the first chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|chapter i  treats...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates a DataFrame with 1 row, which happens to be the\n",
    "# entire row of chapter 1 of Oliver Twist\n",
    "chap1 = spark.read.text(\"data/chap1.txt\")\n",
    "chap1.show()\n",
    "type(chap1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use `split(str, regex, limit)`: Splits str around occurrences that match regex and returns an array with a length of at most limit.\n",
    "Remember we have lazy eval though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'split(value,  , -1)'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lazy eval, so we do not see result\n",
    "split(chap1.value, \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the result of that, we'll use `explode()`: Separates the elements of array expr into multiple rows, or the elements of map expr into multiple rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'explode(split(value,  , -1))'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the array and put it into a DataFrame\n",
    "explode(split(chap1.value, \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the column name isn't great, let's create an alias for the column name so it is easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'explode(split(value,  , -1)) AS word'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode(split(chap1.value, \" \")).alias(\"word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we have a column object that we can select from our original `chap1` data frame.  We need to use the select method.\n",
    "([syntax and more info](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.select.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = explode(split(chap1.value, \" \")).alias(\"word\")\n",
    "\n",
    "# \"Select this column\". Produces one column called \"word\", and the values in it are strings\n",
    "chap1.select(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `.show()` method to actually get the data back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         word|\n",
      "+-------------+\n",
      "|      chapter|\n",
      "|            i|\n",
      "|             |\n",
      "|       treats|\n",
      "|           of|\n",
      "|          the|\n",
      "|        place|\n",
      "|        where|\n",
      "|       oliver|\n",
      "|        twist|\n",
      "|          was|\n",
      "|         born|\n",
      "|          and|\n",
      "|           of|\n",
      "|          the|\n",
      "|circumstances|\n",
      "|    attending|\n",
      "|          his|\n",
      "|        birth|\n",
      "|             |\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = chap1.select(col)\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that was a good check to make sure we had what we wanted.  Now we want to count the number of times each word occurs.  We'll use `groupBy()` and `count()` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string, count: bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `.show()` to execute all the steps above and get something back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|         word|count|\n",
      "+-------------+-----+\n",
      "|         some|    2|\n",
      "|          few|    1|\n",
      "|         hope|    1|\n",
      "|    overseers|    2|\n",
      "|   surrounded|    1|\n",
      "|    biography|    1|\n",
      "|  perspective|    1|\n",
      "|circumstances|    1|\n",
      "|  articulated|    1|\n",
      "|        among|    1|\n",
      "|          day|    1|\n",
      "|         lips|    1|\n",
      "|    appendage|    1|\n",
      "|       raised|    2|\n",
      "|      whether|    1|\n",
      "|          did|    2|\n",
      "|        space|    1|\n",
      "|    existence|    1|\n",
      "|          two|    1|\n",
      "|     instance|    1|\n",
      "|    buildings|    1|\n",
      "|    strangers|    1|\n",
      "|     occurred|    1|\n",
      "|      inmates|    1|\n",
      "|      backand|    1|\n",
      "|       within|    1|\n",
      "|       favour|    1|\n",
      "|        could|    3|\n",
      "|          him|    2|\n",
      "|       badged|    1|\n",
      "+-------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts = words.groupBy(\"word\").count()\n",
    "counts.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's sort it and show some of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  the|   75|\n",
      "|     |   40|\n",
      "|  and|   35|\n",
      "|   of|   35|\n",
      "|    a|   33|\n",
      "|   to|   27|\n",
      "|   in|   22|\n",
      "|  was|   17|\n",
      "|   it|   13|\n",
      "|  her|   13|\n",
      "| that|   12|\n",
      "|  had|   12|\n",
      "| have|   12|\n",
      "|  she|   11|\n",
      "| been|   11|\n",
      "|  his|   11|\n",
      "|   by|   11|\n",
      "|   he|   11|\n",
      "|   on|   10|\n",
      "|which|   10|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts.sort('count', ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
