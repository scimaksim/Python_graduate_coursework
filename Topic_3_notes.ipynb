{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07721657-a8c3-484b-9e07-ec5d5455333e",
   "metadata": {},
   "source": [
    "## Module 11: Kafka & Streaming in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0add32c1-bf28-4b5f-86e5-20039dd2f3d7",
   "metadata": {},
   "source": [
    "We can use Spark (through `pyspark`) to handle streaming data. We do this because Spark is popular and \n",
    "\n",
    "1. Fast, fault-tolerant, works with lots of software\n",
    "2. Can be used with Java, Scala, Python, and R\n",
    "3. Handles big data\n",
    "4. Runs machine learning models on big data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2083b10-a561-44c4-9a46-cecf2b4c6114",
   "metadata": {},
   "source": [
    "Spark SQL DataFrame functions can still be used with streaming to reduce the learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa0ace-78e4-4bf5-9a3d-3fb1d7c35b79",
   "metadata": {},
   "source": [
    "Spark has at least 2 different interface to handle streaming data. \n",
    "\n",
    "1. Spark Streaming (more manual, customizable)\n",
    "    - uses discretized streams, or `DStreams` (internally is a sequence of RDDs, which are the base object that Spark is using for data)\n",
    "2. Spark Structured Streaming (much easier to use)\n",
    "    - can use Spark DataFrames with SQL type functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da442be-352c-47cd-81e2-9b1746ab3ff3",
   "metadata": {},
   "source": [
    "#### Spark SQL Recap (not pandas-on-spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01326898-9ffb-4bd0-8dfd-b917b3ad520e",
   "metadata": {},
   "source": [
    "- Use `pyspark.sql.SparkSession` to create a Spark instance. \n",
    "- DataFrames are created and implemented on top of RDDs (retrieve metadata about a DataFrame using `df.printSchema()`)\n",
    "- DataFrames are stored across the cluster on RDDs\n",
    "    - when _transformations_ are done (e.g. `groupBy()`, `map()`, etc.), lazy evaluation is used (execution does not happen). Common transformations include `.select()` to subset columns, `.withColumn()` to create a new column from another, and `filter()` to subset via a condition. We also have summarizations (often with grouping uses `.groupBy()`) with `.avg()`, `.sum()`, `.count()`, etc.\n",
    "    - when _actions_ (e.g. `show(n)`, `take(n)`, `collect()`) are done, computation starts and results are returned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136495af-8905-407e-a23d-5988df5e020a",
   "metadata": {},
   "source": [
    "Recall that DataFrame and Spark SQL share the same execution engine, so they can be used interchangeably. We can create temporary views using `df.createOrReplaceTempView(\"df\")` and write SQL statements using `spark.sql(\"SELECT sex, age FROM df LIMIT 4\")`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15fe8e-3438-462d-a3ce-691735b24faf",
   "metadata": {},
   "source": [
    "#### Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5f3935-d53f-4a4c-9d47-af938e079392",
   "metadata": {},
   "source": [
    "> Spark Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once processing without the user having to reason about streaming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c627854-30ce-4bee-8e86-9eaf8c2140d5",
   "metadata": {},
   "source": [
    "Spark uses micro-batching at time intervals that you set (100 milliseconds minimum). The general process is \n",
    "\n",
    "1. Create a Spark session (already available when running `pyspark`)\n",
    "2. Read in a stream\n",
    "    - stream from a file, terminal, or use something like Kafka\n",
    "3. Set up transformations/aggregations to do (mostly using SQL type functions)\n",
    "    - perhaps over windows\n",
    "4. Write a query to implement the transformations and define output type\n",
    "    - console (for debugging)\n",
    "    - file (such as .csv)\n",
    "    - database\n",
    "5. The above won't process data until you `.start()` the query!\n",
    "6. Continues for as long as specified or until you terminate it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a5a02a-03d1-4342-b098-9de1a91bb397",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5954ccb-3772-48ff-8cc9-c5e31f80284d",
   "metadata": {},
   "source": [
    "Do a basic word count operation using Spark SQL. To prepare the data, we'll use\n",
    "\n",
    "- `split(str, regex, limit)` - splits `str` around occurrences that match `regex` and returns an array with a length of at most `limit`\n",
    "- `explore(expr)` - separates the elements of an array `expr` into multiple rows, or the elements of map `expr` into multiple rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a231c50-7024-44a0-b7cd-149c6fcd8e63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
