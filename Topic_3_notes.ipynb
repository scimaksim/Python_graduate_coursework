{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07721657-a8c3-484b-9e07-ec5d5455333e",
   "metadata": {},
   "source": [
    "## Module 11: Kafka & Streaming in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0add32c1-bf28-4b5f-86e5-20039dd2f3d7",
   "metadata": {},
   "source": [
    "We can use Spark (through `pyspark`) to handle streaming data. We do this because Spark is popular and \n",
    "\n",
    "1. Fast, fault-tolerant, works with lots of software\n",
    "2. Can be used with Java, Scala, Python, and R\n",
    "3. Handles big data\n",
    "4. Runs machine learning models on big data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2083b10-a561-44c4-9a46-cecf2b4c6114",
   "metadata": {},
   "source": [
    "Spark SQL DataFrame functions can still be used with streaming to reduce the learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa0ace-78e4-4bf5-9a3d-3fb1d7c35b79",
   "metadata": {},
   "source": [
    "Spark has at least 2 different interface to handle streaming data. \n",
    "\n",
    "1. Spark Streaming (more manual, customizable)\n",
    "    - uses discretized streams, or `DStreams` (internally is a sequence of RDDs, which are the base object that Spark is using for data)\n",
    "2. Spark Structured Streaming (much easier to use)\n",
    "    - can use Spark DataFrames with SQL type functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da442be-352c-47cd-81e2-9b1746ab3ff3",
   "metadata": {},
   "source": [
    "#### Spark SQL Recap (not pandas-on-spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01326898-9ffb-4bd0-8dfd-b917b3ad520e",
   "metadata": {},
   "source": [
    "- Use `pyspark.sql.SparkSession` to create a Spark instance. \n",
    "- DataFrames are created and implemented on top of RDDs (retrieve metadata about a DataFrame using `df.printSchema()`)\n",
    "- DataFrames are stored across the cluster on RDDs\n",
    "    - when _transformations_ are done (e.g. `groupBy()`, `map()`, etc.), lazy evaluation is used (execution does not happen). Common transformations include `.select()` to subset columns, `.withColumn()` to create a new column from another, and `filter()` to subset via a condition. We also have summarizations (often with grouping uses `.groupBy()`) with `.avg()`, `.sum()`, `.count()`, etc.\n",
    "    - when _actions_ (e.g. `show(n)`, `take(n)`, `collect()`) are done, computation starts and results are returned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136495af-8905-407e-a23d-5988df5e020a",
   "metadata": {},
   "source": [
    "Recall that DataFrame and Spark SQL share the same execution engine, so they can be used interchangeably. We can create temporary views using `df.createOrReplaceTempView(\"df\")` and write SQL statements using `spark.sql(\"SELECT sex, age FROM df LIMIT 4\")`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15fe8e-3438-462d-a3ce-691735b24faf",
   "metadata": {},
   "source": [
    "#### Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5f3935-d53f-4a4c-9d47-af938e079392",
   "metadata": {},
   "source": [
    "> Spark Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once processing without the user having to reason about streaming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c627854-30ce-4bee-8e86-9eaf8c2140d5",
   "metadata": {},
   "source": [
    "Spark uses micro-batching at time intervals that you set (100 milliseconds minimum). The general process is \n",
    "\n",
    "1. Create a Spark session (already available when running `pyspark`)\n",
    "2. Read in a stream\n",
    "    - stream from a file, terminal, or use something like Kafka\n",
    "3. Set up transformations/aggregations to do (mostly using SQL type functions)\n",
    "    - perhaps over windows\n",
    "4. Write a query to implement the transformations and define output type\n",
    "    - console (for debugging)\n",
    "    - file (such as .csv)\n",
    "    - database\n",
    "5. The above won't process data until you `.start()` the query!\n",
    "6. Continues for as long as specified or until you terminate it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a5a02a-03d1-4342-b098-9de1a91bb397",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5954ccb-3772-48ff-8cc9-c5e31f80284d",
   "metadata": {},
   "source": [
    "Do a basic word count operation using Spark SQL. To prepare the data, we'll use\n",
    "\n",
    "- `split(str, regex, limit)` - splits `str` around occurrences that match `regex` and returns an array with a length of at most `limit`\n",
    "- `explore(expr)` - separates the elements of an array `expr` into multiple rows, or the elements of map `expr` into multiple rows and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b16d5-b445-4af6-b5c3-fe2bb558a819",
   "metadata": {},
   "source": [
    "Once the data is prepped, we'll count the occurrences:\n",
    "\n",
    "- use `.groupBy()` with `.count()`\n",
    "\n",
    "The syntax will look like `df.select([\"Duration\", \"Age\", \"Treatment\"]).groupBy(\"Treatment\").avg().show()`. We won't be able to use `.show()` because we're dealing with streaming data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b3f51b-33ac-4ec0-a33c-51ad66d8ecaf",
   "metadata": {},
   "source": [
    "##### Plan (try this with just Spark, no streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33270657-78fb-4317-9bb5-01dbfd90e29f",
   "metadata": {},
   "source": [
    "1. Start a Spark session\n",
    "2. Create a Spark DataFrame from an input data set (strings with words separated by spaces)\n",
    "3. Split up the strings using `split()` and `explode()`\n",
    "4. Count the number of times each word appears using `groupBy()` and `count()`\n",
    "5. Result should be words with their associated counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca64306a-e64d-4462-9781-82a27e42c9b2",
   "metadata": {},
   "source": [
    "See the file `StreamingData_wordCountSparkSQL.ipynb` for an implementation of this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381499fa-640e-4ce2-8c98-a54b4016d2da",
   "metadata": {},
   "source": [
    "#### Example using Streaming Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7220f-4717-461e-aa61-5d44ead141cc",
   "metadata": {},
   "source": [
    "To set up a stream, we need a streaming source. One such source is Kafka, which can stream data using producers. The steps are\n",
    "\n",
    "1. Start up Zookeeper\n",
    "2. Start Kafka\n",
    "3. Create a Kafka topic\n",
    "4. Create a Kafka producer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ec96d-aa65-412d-b511-3ec493f53ef6",
   "metadata": {},
   "source": [
    "Next, we follow these steps:\n",
    "\n",
    "1. Start pyspark\n",
    "2. Read in the Kafka stream (`.readStream`)\n",
    "3. Process it to count words (`split()` and `explode()` with `groupBy()` and `count()`)\n",
    "4. Write the output to the console (`.writeStream`)\n",
    "5. Start the query and begin sending data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6facc9c-6f04-4e6a-84bc-26e29c712276",
   "metadata": {},
   "source": [
    "On a Mac M1 (Apple Silicon) you can install Kafka using `brew` with \n",
    "\n",
    "`$ brew install kafka`\n",
    "\n",
    "You can start Zookeeper with \n",
    "\n",
    "`$ zookeeper-server-start /opt/homebrew/etc/kafka/zookeeper.properties`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0891098e-8c16-46ad-a35e-11560aac12d0",
   "metadata": {},
   "source": [
    "You can then start Kafka with\n",
    "\n",
    "`$ kafka-server-start /opt/homebrew/etc/kafka/server.properties`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c79004-ccf4-4518-8d97-36f5611253a3",
   "metadata": {},
   "source": [
    "We then need to create a Kafka _topic_ and a Kafka _producer_ that will send out data. The line below creates the topic:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8976c0-f8c8-466b-9eb5-78c67d21c8b6",
   "metadata": {},
   "source": [
    "`$ kafka-topics --create -topic mystream --bootstrap-server localhost:9092`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49afc000-945b-4692-81b7-5d8596a087b3",
   "metadata": {},
   "source": [
    "The line below creates the producer:\n",
    "\n",
    "`$ kafka-console-producer --topic mystream --bootstrap-server localhost:9092`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea73c5c-505c-446b-86fe-730a9a8cde4f",
   "metadata": {},
   "source": [
    "Then, we can run\n",
    "\n",
    "`$ pyspark --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.2.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be8301-0c26-4038-b6a8-cfb9db512b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
